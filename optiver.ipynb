{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-10T17:57:58.406686Z","iopub.execute_input":"2023-11-10T17:57:58.407306Z","iopub.status.idle":"2023-11-10T17:57:58.775947Z","shell.execute_reply.started":"2023-11-10T17:57:58.407274Z","shell.execute_reply":"2023-11-10T17:57:58.774986Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n/kaggle/input/optiver-trading-at-the-close/train.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Sequence, Tuple\n\nimport pandas as pd\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nclass MockApi:\n    def __init__(self):\n        self.input_paths: Sequence[str] = ['/kaggle/input/inputapi/Excel11.xlsx']  # actual path\n        self.group_id_column: str = \"group_id\"  # Placeholder column name\n        self.export_group_id_column: bool = False\n        assert len(self.input_paths) >= 2\n\n        self._status = 'initialized'\n        self.predictions = []\n\n    def iter_test(self) -> Tuple[pd.DataFrame]:\n        if self._status != 'initialized':\n            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n\n        dataframes = []\n        for pth in self.input_paths:\n            dataframes.append(pd.read_csv(pth, low_memory=False))\n        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n\n        for group_id in group_order:\n            self._status = 'prediction_needed'\n            current_data = []\n            for df in dataframes:\n                cur_df = df.loc[group_id].copy()\n                if not isinstance(cur_df, pd.DataFrame):\n                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)},index=[group_id])\n                    cur_df.index.name = self.group_id_column\n                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n                current_data.append(cur_df)\n            yield tuple(current_data)\n\n            while self._status != 'prediction_received':\n                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n                yield None\n\n        # Update the path where the submission file is saved\n        submission_path = '/kaggle/working/submission.csv'\n        with open(submission_path, 'w') as f_open:\n            pd.concat(self.predictions).to_csv(f_open, index=False)\n        self._status = 'finished'\n\n    def predict(self, user_predictions: pd.DataFrame):\n        if self._status == 'finished':\n            raise Exception('You have already made predictions for the full test set.')\n        if self._status != 'prediction_needed':\n            raise Exception('You must get the next test sample from `iter_test()` first.')\n        if not isinstance(user_predictions, pd.DataFrame):\n            raise Exception('You must provide a DataFrame.')\n\n        self.predictions.append(user_predictions)\n        self._status = 'prediction_received'\n\n\ndef make_env():\n    return MockApi()\n\n# Define the paths to the input files\ntrain_file = os.path.join('/kaggle/input/optiver-trading-at-the-close/train.csv')\ntest_file = os.path.join('/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv')\nsample_submission_file = os.path.join('/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv')\nrevealed_targets_file = os.path.join('/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\n\n# Step 1: Data Preparation\ntrain_data = pd.read_csv(train_file)\ntest_data = pd.read_csv(test_file)\nsample_submission = pd.read_csv(sample_submission_file)\nrevealed_targets = pd.read_csv(revealed_targets_file)\n\n# Step 2: Preprocess Data\ntrain_data.fillna(0, inplace=True)\ntest_data.fillna(0, inplace=True)\n\n# Step 3: Feature Engineering\ndef calculate_synthetic_index(data):\n    if 'synthetic_index' in data.columns:\n        data['synthetic_index'] = ((data['wap'].shift(-60) / data['wap'])) * 10000\n    return data\n\nif 'synthetic_index' in train_data.columns:\n    train_data = train_data.groupby(['stock_id', 'date_id']).apply(calculate_synthetic_index)\n    train_data = train_data.drop(columns=['synthetic_index'])\n\nif 'synthetic_index' in test_data.columns:\n    test_data = test_data.groupby(['stock_id', 'date_id']).apply(calculate_synthetic_index)\n    test_data = test_data.drop(columns=['synthetic_index'])\n\n# Step 4: Model Selection\nX = train_data.drop(['target'], axis=1)\ny = train_data['target']\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(32, activation='relu', input_shape=(X.shape[1], 1)),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n# Step 5: Training the Model\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_train_scaled = X_train_scaled[:, :, np.newaxis]\n\nX_valid_scaled = scaler.transform(X_valid)\nX_valid_scaled = X_valid_scaled[:, :, np.newaxis]\n\nhistory = model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_valid_scaled, y_valid), batch_size=64)\n\nvalidation_loss = model.evaluate(X_valid_scaled, y_valid)\nprint(f'Validation Loss: {validation_loss}')\n\ntest_data_scaled = scaler.transform(test_data)\ntest_data_scaled = test_data_scaled[:, :, np.newaxis]\n\n# Step 7: Prepare Output for Submission\nenv = make_env()\niter_test = env.iter_test()\n\nfor (test, _, sample_prediction) in iter_test:\n    if 'synthetic_index' in test.columns:\n        test = test.groupby(['stock_id', 'date_id']).apply(calculate_synthetic_index)\n        test = test.drop(columns=['synthetic_index'])\n\n    test_data_scaled = scaler.transform(test)\n    test_data_scaled = test_data_scaled[:, :, np.newaxis]\n\n    predictions = model.predict(test_data_scaled)\n\n    sample_prediction['target'] = predictions.flatten()\n    env.predict(sample_prediction)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T18:17:07.508414Z","iopub.execute_input":"2023-11-10T18:17:07.509078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}